{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Assignment 3","provenance":[{"file_id":"16FbvI3qe2HoNSpfj1yOk_pSvLnq-rje8","timestamp":1582371959090},{"file_id":"1t5BCLoaBIoK_ksBCkhCnW-qJ20E0Ns7-","timestamp":1582360732443}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"P5y9270HT2uv","colab_type":"text"},"source":["### Donwloading Dataset"]},{"cell_type":"code","metadata":{"id":"lCpBKWFjqtYU","colab_type":"code","colab":{}},"source":["!pip install kaggle\n","!mkdir .kaggle"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0x0avkMQsNyf","colab_type":"code","colab":{}},"source":["import json\n","token = {\"username\":\"ystark\",\"key\":\"7d9d124dcd4afabb3ef553b00daea011\"}\n","with open('/content/.kaggle/kaggle.json', 'w') as file:\n","    json.dump(token, file)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tEL5dHywD348","colab_type":"code","colab":{}},"source":["#May require to run twice\n","!cp /content/.kaggle/kaggle.json ~/.kaggle/kaggle.json;\n","!kaggle config set -n path -v {/content};"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GIUeFEvnsWHt","colab_type":"code","colab":{}},"source":["!chmod 600 /root/.kaggle/kaggle.json;\n","!kaggle competitions download -c twitter-sentiment-analysis2;\n","\n","!unzip /content/{/content}/competitions/twitter-sentiment-analysis2/train.csv.zip;\n","!unzip /content/{/content}/competitions/twitter-sentiment-analysis2/test.csv.zip;"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-vDDTnitMNh","colab_type":"text"},"source":["# Begin Project"]},{"cell_type":"markdown","metadata":{"id":"CHwHp2PNTanh","colab_type":"text"},"source":["file descriptions\n","\n","train.csv - the training set\n","test.csv - the test set\n","Data fields\n","\n","ItemID - id of twit\n","Sentiment - sentiment\n","SentimentText - text of the twit\n","\n","\n","0 - negative\n","\n","1 - positive"]},{"cell_type":"code","metadata":{"id":"PmvfvDHNe6h5","colab_type":"code","colab":{}},"source":["import pandas as pd\n","import numpy as np\n","from matplotlib import pyplot as plt\n","import seaborn as sns\n","import string \n","import re\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import TweetTokenizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ajerTCq7UHHb","colab":{}},"source":["train_data=pd.read_csv('/content/train.csv',encoding = \"ISO-8859-1\")\n","train_data.head()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2nKoR89bsAAS","colab_type":"text"},"source":["### Preprocessing"]},{"cell_type":"code","metadata":{"id":"pc0YS8oyw2cc","colab_type":"code","colab":{}},"source":["tweets=pd.DataFrame(train_data.SentimentText)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5qg37uIoJfm","colab_type":"code","colab":{}},"source":["tt = TweetTokenizer()\n","tweets['tokenized_text']=tweets.apply(lambda row: tt.tokenize(row['SentimentText']),axis=1)\n","tweets['joined_prepr_text'] = tweets['tokenized_text'].apply(lambda x: ' '.join(map(str, x)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cPRiLBXwH2z","colab_type":"code","colab":{}},"source":["tweets.head()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4Fll4CVKIfxw","colab_type":"code","colab":{}},"source":["vocab_size=0\n","for i in range(len(tweets)):\n","  vocab_size+=len(tweets.tokenized_text[i])\n","\n","print('Vocab_size: ',vocab_size)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cCl-xySHKbk_","colab_type":"text"},"source":["### Create Train-Test Set"]},{"cell_type":"code","metadata":{"id":"nPR8a4DJy_ZD","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ul1sgqg30ATE","colab_type":"code","colab":{}},"source":["X_train,X_test,y_train,y_test=train_test_split(tweets['joined_prepr_text'],train_data['Sentiment'],test_size = 0.2,random_state=1)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"exbxsir2G8F-","colab_type":"code","colab":{}},"source":["X_val.shape"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CTc-KpV-fvI8","colab_type":"code","colab":{}},"source":["print(X_train.shape, X_test.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_0c1d-CjRi1Y","colab_type":"text"},"source":["#### Model Training"]},{"cell_type":"code","metadata":{"id":"NbxJT67HRlnz","colab_type":"code","colab":{}},"source":["def recall(y_true, y_pred):\n","    \n","    \"\"\"\n","    Recall metric.\n","    Only computes a batch-wise average of recall.\n","    Computes the recall, a metric for multi-label classification of\n","    how many relevant items are selected.\n","    \"\"\"\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    return recall\n","\n","\n","def precision(y_true, y_pred):\n","    \n","    \"\"\"\n","    Precision metric.\n","    Only computes a batch-wise average of precision.\n","    Computes the precision, a metric for multi-label classification of\n","    how many selected items are relevant.\n","    Source\n","    ------\n","    https://github.com/fchollet/keras/issues/5400#issuecomment-314747992\n","    \"\"\"\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    return precision\n","\n","\n","def f1(y_true, y_pred):\n","    \n","    \"\"\"Calculate the F1 score.\"\"\"\n","    p = precision(y_true, y_pred)\n","    r = recall(y_true, y_pred)\n","    return 2 * ((p * r) / (p + r))\n","\n","\n","def accuracy(y_true, y_pred):\n","    return K.mean(K.equal(y_true, K.round(y_pred)), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xp_UG6AJQDRu","colab_type":"code","colab":{}},"source":["import warnings\n","import sklearn.exceptions\n","warnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n","warnings.simplefilter(action='ignore', category=FutureWarning)\n","\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.python.keras.layers import Dropout, Dense\n","from tensorflow.khttps://machinelearningmastery.com/save-load-keras-deep-learning-models/eras.optimizers import Adam\n","from tensorflow.keras import backend as K\n","\n","n_classes = 1\n","\n","model2 = Sequential()\n","model2.add(Dense(512, input_dim=X_train_cent.shape[1] , activation='relu'))\n","model2.add(Dropout(0.3))\n","# model2.add(Dense(1024,  activation='relu'))\n","# model2.add(Dropout(0.5))\n","model2.add(Dense(124,  activation='relu'))\n","model2.add(Dropout(0.5))\n","model2.add(Dense(n_classes,  activation='sigmoid'))\n","\n","print(model2.summary())\n","model2.compile(loss='binary_crossentropy',\n","                  optimizer=Adam(lr=0.001),\n","                  metrics=[precision, recall, f1, accuracy])\n","\n","checkpoint2 = ModelCheckpoint('keras_fastetxt_centroids_model', monitor='val_f1', verbose=1, save_best_only=True, mode='max')\n","\n","history2 = model2.fit(X_train_cent, y_train,\n","              batch_size=32,\n","              epochs=15,\n","              verbose = 1,\n","              callbacks=[checkpoint2],\n","              validation_data=(X_val_cent, y_val),\n","              shuffle=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HmBlyW0ce9_t","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","\n","\n","# summarize history for f1\n","plt.plot(history2.history['f1'])\n","plt.plot(history2.history['val_f1'])\n","plt.title('model f1')\n","plt.ylabel('f1-score')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper left')\n","plt.show()\n","# summarize history for loss\n","plt.plot(history2.history['loss'])\n","plt.plot(history2.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'dev'], loc='upper right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_IOUhArmBsEL","colab_type":"text"},"source":["### Results"]},{"cell_type":"markdown","metadata":{"id":"5JDfRrcpxTNU","colab_type":"text"},"source":["#### Dummy Classifier\n"]},{"cell_type":"code","metadata":{"id":"TibxaEUq30a5","colab_type":"code","colab":{}},"source":["from sklearn.dummy import DummyClassifier\n","from sklearn.metrics import f1_score\n","from sklearn.metrics import precision_score\n","from sklearn.metrics import recall_score\n","\n","results = {}\n","\n","base = DummyClassifier(strategy='most_frequent')\n","base.fit(X_train_tfidf, y_train)\n","\n","predictions = base.predict(X_train_tfidf)\n","results['f1-score train'] = f1_score(y_train, predictions)\n","results['f1-score macro train'] = f1_score(y_train, predictions, average='macro')\n","results['precision train'] = precision_score(y_train, predictions)\n","results['recall train'] = recall_score(y_train, predictions)\n","print(\"train f1-score: %.2f%%\" % (results['f1-score train']*100))\n","print(\"train macro f1-score: %.2f%%\" % (results['f1-score macro train']*100))\n","print(\"train precision: %.2f%%\" % (results['precision train']*100))\n","print(\"train recall: %.2f%%\" % (results['recall train']*100))\n","\n","predictions_test = base.predict(X_test_tfidf)\n","results['f1-score test'] = f1_score(y_test, predictions_test)\n","results['f1-score macro test'] = f1_score(y_test, predictions_test, average='macro')\n","results['precision test'] = precision_score(y_test, predictions_test)\n","results['recall test'] = recall_score(y_test, predictions_test)\n","print(\"test f1-score: %.2f%%\" % (results['f1-score test']*100))\n","print(\"test macro f1-score: %.2f%%\" % (results['f1-score macro test']*100))\n","print(\"test precision: %.2f%%\" % (results['precision test']*100))\n","print(\"test recall: %.2f%%\" % (results['recall test']*100))\n","print()\n","print(\"test data confusion matrix\")\n","y_true = pd.Series(y_test, name='True')\n","y_pred = pd.Series(predictions_test, name='Predicted')\n","pd.crosstab(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6TS-apPoZ3Bq","colab_type":"text"},"source":["### MLP"]},{"cell_type":"code","metadata":{"id":"DvfxipIdP0e1","colab_type":"code","colab":{}},"source":["predictions = model2.predict(X_train_cent)\n","predictions = np.where(predictions>0.5, 1, predictions)\n","predictions = np.where(predictions<0.5, 0, predictions)\n","results['f1-score train'] = f1_score(y_train, predictions)\n","results['f1-score macro train'] = f1_score(y_train, predictions, average='macro')\n","results['precision train'] = precision_score(y_train, predictions)\n","results['recall train'] = recall_score(y_train, predictions)\n","print(\"train f1-score: %.2f%%\" % (results['f1-score train']*100))\n","print(\"train macro f1-score: %.2f%%\" % (results['f1-score macro train']*100))\n","print(\"train precision: %.2f%%\" % (results['precision train']*100))\n","print(\"train recall: %.2f%%\" % (results['recall train']*100))\n","\n","predictions_test = model2.predict(X_test_cent)\n","predictions_test = np.where(predictions_test>0.5, 1, predictions_test)\n","predictions_test = np.where(predictions_test<0.5, 0, predictions_test)\n","results['f1-score test'] = f1_score(y_test, predictions_test)\n","results['f1-score macro test'] = f1_score(y_test, predictions_test, average='macro')\n","results['precision test'] = precision_score(y_test, predictions_test)\n","results['recall test'] = recall_score(y_test, predictions_test)\n","print(\"test f1-score: %.2f%%\" % (results['f1-score test']*100))\n","print(\"test macro f1-score: %.2f%%\" % (results['f1-score macro test']*100))\n","print(\"test precision: %.2f%%\" % (results['precision test']*100))\n","print(\"test recall: %.2f%%\" % (results['recall test']*100))\n","print()\n","print(\"test data confusion matrix\")\n","y_true = pd.Series(y_test, name='True')\n","y_pred = pd.Series(predictions_test.ravel(), name='Predicted')\n","pd.crosstab(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0aOaxoKaOd0s","colab_type":"text"},"source":["#### Multinomial NB "]},{"cell_type":"code","metadata":{"id":"l8T2HAtGxHTN","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import LogisticRegression\n","from sklearn.naive_bayes import MultinomialNB\n","\n","NB = MultinomialNB(alpha=1.1,fit_prior=True)\n","\n","#instantiate X_train feature vector according to the best tf-idf parameters fitted for NB\n","tfidf_NB=TfidfVectorizer(ngram_range=(1,2),max_features = 10000,binary=False,norm='l2',max_df=1.0)\n","X_train_tfidf_NB=tfidf_NB.fit_transform(X_train)\n","X_test_tfidf_NB=tfidf_NB.transform(X_test)\n","\n","NB.fit(X_train_tfidf_NB, y_train)\n","results = {}\n","\n","predictions = NB.predict(X_train_tfidf_NB)\n","results['f1-score train'] = f1_score(y_train, predictions)\n","results['f1-score macro train'] = f1_score(y_train, predictions, average='macro')\n","results['precision train'] = precision_score(y_train, predictions)\n","results['recall train'] = recall_score(y_train, predictions)\n","print(\"train f1-score: %.2f%%\" % (results['f1-score train']*100))\n","print(\"train macro f1-score: %.2f%%\" % (results['f1-score macro train']*100))\n","print(\"train precision: %.2f%%\" % (results['precision train']*100))\n","print(\"train recall: %.2f%%\" % (results['recall train']*100))\n","\n","predictions_test = NB.predict(X_test_tfidf_NB)\n","results['f1-score test'] = f1_score(y_test, predictions_test)\n","results['f1-score macro test'] = f1_score(y_test, predictions_test, average='macro')\n","results['precision test'] = precision_score(y_test, predictions_test)\n","results['recall test'] = recall_score(y_test, predictions_test)\n","print(\"test f1-score: %.2f%%\" % (results['f1-score test']*100))\n","print(\"test macro f1-score: %.2f%%\" % (results['f1-score macro test']*100))\n","print(\"test precision: %.2f%%\" % (results['precision test']*100))\n","print(\"test recall: %.2f%%\" % (results['recall test']*100))\n","print()\n","print(\"test data confusion matrix\")\n","y_true = pd.Series(y_test, name='True')\n","y_pred = pd.Series(predictions_test, name='Predicted')\n","pd.crosstab(y_true, y_pred)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YvFmj6WmP8-t","colab_type":"text"},"source":["### Learning Curves "]},{"cell_type":"code","metadata":{"id":"SNzwu-EE5c4X","colab_type":"code","colab":{}},"source":["#Learning curves with cross-validtion\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import learning_curve\n","from sklearn.model_selection import ShuffleSplit\n","\n","def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n","                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n","   \n","    plt.figure()\n","    plt.title(title)\n","    if ylim is not None:\n","        plt.ylim(*ylim)\n","    plt.xlabel(\"Training examples\")\n","    plt.ylabel(\"F1-score\")\n","    train_sizes, train_scores, test_scores = learning_curve(\n","        estimator, X, y, cv=cv, n_jobs=n_jobs,scoring='f1', train_sizes=train_sizes)\n","    train_scores_mean = np.mean(train_scores, axis=1)\n","    train_scores_std = np.std(train_scores, axis=1)\n","    test_scores_mean = np.mean(test_scores, axis=1)\n","    test_scores_std = np.std(test_scores, axis=1)\n","    plt.grid()\n","\n","    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n","                     train_scores_mean + train_scores_std, alpha=0.1,\n","                     color=\"b\")\n","    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n","                     test_scores_mean + test_scores_std, alpha=0.1, color=\"orange\")\n","    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"b\",\n","             label=\"Training score\")\n","    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"orange\",\n","             label=\"Cross-validation score\")\n","\n","    plt.legend(loc=\"lower right\")\n","    return plt\n","\n","\n","X, y = X_train_tfidf_LR,y_train\n","\n","\n","title = \"Learning Curves (Logistic Regression)\"\n","cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n","plot_learning_curve(lr, title, X_train_tfidf_LR, y, (0.7, 1.01), cv=cv, n_jobs=-1)\n","plt.show()\n","\n","\n","title = \"Learning Curves (Naive Bayes)\"\n","cv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n","# estimator = MultinomialNB()\n","plot_learning_curve(NB, title, X_train_tfidf_NB, y, ylim=(0.7, 1.01), cv=cv, n_jobs=2)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nS0c-unG5qgq","colab_type":"code","colab":{}},"source":["#Precision-Recall curves\n","from sklearn.metrics import precision_recall_curve\n","from sklearn.metrics import auc\n","\n","\n","#disable warnings for large float numbers of estimated probabilities\n","np.seterr(all='ignore')\n","estimators = {'Logistic Regression':lr,\n","              'Naive Bayes':NB}\n","\n","for (name,estimator) in estimators.items():\n","    \n","    model =estimator\n","    if(name=='Logistic Regression'):\n","      pred = model.predict_proba(X_test_tfidf_LR)\n","    elif(name=='Naive Bayes'):\n","      pred = model.predict_proba(X_test_tfidf_NB)\n","      \n","    precision, recall, thresholds = precision_recall_curve(y_test, pred[:,1])\n","    area = auc(recall, precision)\n","\n","    plt.plot(recall, precision, label='Precision-Recall curve')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.ylim([0.0, 1.05])\n","    plt.xlim([0.0, 1.0])\n","    plt.title('Precision-Recall %s: AUC=%0.2f' % (name,area))\n","    plt.legend(loc=\"lower left\")\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HqwbZggFaRKI","colab_type":"text"},"source":["### Bootstrap"]},{"cell_type":"code","metadata":{"id":"5grPjnHlMN5K","colab_type":"code","colab":{}},"source":["from tqdm import tqdm\n","from sklearn.utils import resample\n","\n","def bootstrap_test(n_iterations, n_size, X, y, model1, model2):\n","  #this function was based on slide 31\n","  #get predictions on the whole test set for the two models\n","  initial_predictions_model1 = model1.predict(X)\n","  initial_predictions_model2 = model2.predict(X)\n","  initial_model1_f1score = f1_score(y, initial_predictions_model1, average='macro')\n","  initial_model2_f1score = f1_score(y, initial_predictions_model2, average='macro')\n","  #estimate δ(x) for whole test set\n","  initial_dif = initial_model1_f1score - initial_model2_f1score\n","  #setup b\n","  bsets = 0\n","  my_list=list(range(n_iterations))\n","  for i in tqdm(my_list):\n","    #sample with replacement the test set\n","    X_sub, y_sub = resample(X, y, n_samples=n_size)\n","    #get predictions on the new sub-set for the two models\n","    subsample_predictions_model1 = model1.predict(X_sub)\n","    subsample_predictions_model2 = model2.predict(X_sub)\n","    subsample_model1_f1score = f1_score(y_sub, subsample_predictions_model1, average='macro')\n","    subsample_model2_f1score = f1_score(y_sub, subsample_predictions_model2, average='macro')\n","    #estimate δ(x*(i)) for new sub-set test set\n","    subsample_dif = subsample_model1_f1score - subsample_model2_f1score\n","    if subsample_dif > 2 * initial_dif:\n","      bsets +=1\n","    p_value = bsets / n_iterations\n","  return p_value"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f_H2vpf-VgE","colab_type":"code","colab":{}},"source":["# configure bootstrap\n","n_iterations = 10000\n","n_size = len(y_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cno5EPTQERV","colab_type":"code","colab":{}},"source":["# run bootstrap between LR and dummy classifier\n","bootstrap_test(n_iterations, n_size, X_test_tfidf_LR, y_test, lr, base)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0aGb7ygsfhvg","colab_type":"code","colab":{}},"source":["# run bootstrap between LR and Naive Bayes\n","bootstrap_test(n_iterations, n_size, X_test_tfidf_LR, y_test, lr, NB)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjzlR1mM08ft","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}